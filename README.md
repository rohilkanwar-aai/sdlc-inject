# SDLC Inject

CLI tool for building realistic, long-horizon debugging environments that test AI agents' ability to investigate and resolve production incidents. Built for Anthropic's SDLC Environments RFP.

## How It Works

We inject realistic failure patterns into real, human-written codebases and surround them with evidence-seeded MCP tool servers that simulate the observability stack (Slack, Sentry, Prometheus, PagerDuty, logs, etc.). The agent investigates using the same tools a human engineer would use.

**The difficulty is in the investigation, not the fix.** A cascading failure might resolve with a one-line config change -- but tracing from the user-visible symptom through 8-30 logical hops to find that config is the multi-hour task.

### Three Modes of Operation

1. **Static Evidence** (CASCADE-009/012): Pre-seeded YAML data returned by MCP tools. Fastest setup, deterministic.
2. **LLM-Powered Coworkers** (CASCADE-011): Coworker responses generated by Sonnet 4.5 with partial domain knowledge. Non-deterministic, contextual.
3. **Real-Time Traffic Simulation** (CASCADE-011+): Background daemon generates live traffic data (logs, metrics, Sentry events) into SQLite. MCP servers read from the continuously-growing database. Most realistic.

### Architecture

```
                    +-----------------------+
                    |   Target Codebase     |
                    |  (OTel Demo, Train    |
                    |   Ticket, etc.)       |
                    |  + injected bugs      |
                    +-----------+-----------+
                                |
                    +-----------v-----------+
                    |   Evidence Map YAML   |
                    |  (per-hop evidence,   |
                    |   red herrings,       |
                    |   noise config)       |
                    +-----------+-----------+
                                |
            +-------------------+-------------------+
            |                   |                   |
    +-------v------+   +-------v------+   +--------v-----+
    | Noise Engine  |   | Reactive     |   | Timeline     |
    | (50K+ entries |   | Slack        |   | Simulation   |
    |  per source)  |   | (Q&A pairs,  |   | (chaos monkey|
    |               |   |  delays,     |   |  false fixes |
    |               |   |  buy-in)     |   |  metrics     |
    +-------+------+   +-------+------+   |  degradation)|
            |                   |          +--------+-----+
            +-------------------+-------------------+
                                |
                    +-----------v-----------+
                    |   MCP Server (stdio)  |
                    |   42 tools:           |
                    |   Slack, Sentry,      |
                    |   Prometheus, kubectl, |
                    |   Jira, Confluence,   |
                    |   Datadog, etc.       |
                    |   + 20% rate limiting |
                    +-----------+-----------+
                                |
                    +-----------v-----------+
                    |   Claude Code Agent   |
                    |   (investigates the   |
                    |    incident)          |
                    +-----------------------+
```

## Features

### Pattern Catalog (31 patterns)
- **CASCADE-001 to CASCADE-012**: Cascading failures (6-30 hops, 4-15% pass rate)
- **RACE-001 to RACE-005**: Race conditions
- **SPLIT-001 to SPLIT-005**: Split-brain scenarios
- **CLOCK-001 to CLOCK-005**: Clock skew issues
- **COORD-001 to COORD-005**: Coordination failures

### Cascade Pattern Matching
```bash
# Find which cascade patterns fit a codebase
sdlc-inject cascade-fit ./my-microservice-app
sdlc-inject cascade-fit https://github.com/open-telemetry/opentelemetry-demo
```

### Noise Generation Engine
- 50,000+ Slack messages per channel with buried signal entries
- 50,000+ log entries per service with realistic level distributions
- 20 user personas with distinct speech patterns
- Cursor-based pagination for efficient access to large datasets

### Reactive Slack (Interactive Coworkers)
- `slack_post_message` triggers contextual coworker responses
- Multi-turn conversations (coworkers give different answers on 2nd/3rd ask)
- Response delays (Kevin takes 3 tool calls to respond)
- Adversarial-in-good-faith responses (coworkers push wrong theories)
- Buy-in requirement (must get team review before deploying fixes)
- False fix detection (coworkers celebrate then report regression)

### Time-Progressing Simulation
- Metrics worsen with each tool call
- New Slack messages appear at time milestones
- Chaos monkey: new "incidents" fire during investigation (all symptoms of root cause)
- Recovery attempts create brief improvements then re-degrade
- VP demands status updates under time pressure

### Difficulty Amplifiers
1. **Adversarial coworkers** -- redirect investigation toward wrong bugs
2. **Hidden metrics** -- direct Kafka metrics return "NO DATA" (collector is backed up)
3. **Misleading first results** -- PagerDuty shows "RESOLVED", Sentry shows wrong project first
4. **False fix trap** -- fixing one symptom works temporarily, then fails
5. **MCP rate limiting** -- 20% of tool calls rejected, verbose metadata bloats context
6. **Confusing naming** -- same service called 4 different names across tools
7. **Chaos monkey** -- concurrent breaking events that are cascade symptoms

### LLM-Powered Coworkers (Sonnet 4.5)
- Each coworker is a **persona** with role, experience, personality, biases
- **Partial domain knowledge**: each person knows their area, can't confirm what they don't know
- **Conversation history**: maintains coherent multi-turn per-persona interactions
- **No one knows the root cause**: agent must piece it together from tools + code
- **Adversarial-in-good-faith**: coworkers push wrong theories based on their biases
- Tyler has the right intuition but nobody listens to him

### Real-Time Traffic Simulator
- **Background daemon thread** generates ~100 req/sec of production traffic
- **SQLite database** stores logs, metrics, Sentry events, Slack messages
- **Cascade events** (1% probability/sec): large batch → 4MB truncation → duplicate webhook → negative inventory
- **Pre-seeds 30 minutes of history** so agent sees recent data on first query
- **Metrics change organically** between tool calls -- not just per-tick modifiers
- **New alerts appear** when thresholds are crossed
- MCP servers **read from SQLite** instead of static YAML

## Quick Start

### 1. Install
```bash
git clone <repo>
cd sdlc-inject
python -m venv .venv && source .venv/bin/activate
pip install -e .
```

### 2. Clone a Target Codebase
```bash
git clone --depth 1 https://github.com/open-telemetry/opentelemetry-demo ./opentelemetry-demo
# or
git clone --depth 1 https://github.com/FudanSELab/train-ticket ./train-ticket
```

### 3. Find Best Pattern Match
```bash
sdlc-inject cascade-fit ./opentelemetry-demo --top-k 5
```

### 4. Inject a Pattern
Modify the target codebase files as specified in the pattern YAML (see `patterns/cascade/CASCADE-012.yaml` for an example of the injection points).

### 5. Set Up Evidence
```bash
# Evidence map + MCP server are in evaluation/cascade-012/
ls evaluation/cascade-012/
# CASCADE-012-evidence-map.yaml  -- all evidence across 7 MCP sources
# mcp_evidence_server.py         -- stdio MCP server for Claude Code
```

### 6. Test with Claude Code
```bash
cd ./train-ticket  # or ./opentelemetry-demo
# .mcp.json and CLAUDE.md should already be configured
claude
```

## Project Structure

```
sdlc-inject/
  patterns/                       # Pattern catalog (31 YAML definitions)
    cascade/                      # CASCADE-001 through CASCADE-012
    race/                         # RACE-001 through RACE-005
    split-brain/                  # SPLIT-001 through SPLIT-005
    clock-skew/                   # CLOCK-001 through CLOCK-005
    coordination/                 # COORD-001 through COORD-005

  evaluation/                     # Per-pattern evaluation configs
    cascade-009/                  # Evidence map + MCP server
    cascade-012/                  # Evidence map + MCP server (hardened)

  sdlc_inject/                    # Python implementation
    mcp_servers/
      evidence.py                 # Evidence-seeded MCP servers
      noise.py                    # Noise generation engine (50K+ entries)
      reactive.py                 # Reactive Slack with Q&A + delays + buy-in
      timeline.py                 # Time-progressing simulation + chaos monkey
      llm_coworkers.py            # LLM-powered coworker personas (Sonnet 4.5)
      db_backed.py                # SQLite-backed MCP server helpers
      base.py                     # Base MCP server interface
      generic.py                  # Template-based generic server
      sentry.py, slack.py, ...    # Per-tool mock servers

    simulator/
      traffic.py                  # Real-time traffic generator (daemon thread + SQLite)

    cascade_matcher.py            # Codebase-to-pattern matching
    injection.py                  # Pattern injection engine
    multi_pattern.py              # Multi-pattern orchestration
    analyzer/                     # Neural codebase analysis (Claude Agent SDK)
    artifacts/                    # Mock artifact generators
    harness/                      # Parallel agent evaluation
    discovery/                    # Dynamic tool discovery from incidents

  injection_configs/              # Multi-pattern combination configs
```

## RFP Alignment

This tool directly addresses Anthropic's SDLC Environments RFP requirements:

| RFP Requirement | How We Address It |
|---|---|
| Realistic, long-horizon tasks | 30-hop cascading failures across 47 microservices |
| Docker-compatible | Evidence-seeded MCP servers run in any container |
| Locked down | Agent cannot see evidence maps or golden paths |
| Context-complete | All evidence accessible via MCP tools |
| <50% frontier pass rate | Tested: Opus scores ~25-35% on hardened CASCADE-012 |
| Grading | Hybrid outcome + process grading per pattern |
| Multiple SDLC phases | Debugging + Maintenance + Communication + Verification |

### Tested Pass Rates

| Pattern | Hops | Codebase | Pass Rate | Notes |
|---|---|---|---|---|
| CASCADE-009 (v1) | 8 | OTel Demo (212 files) | ~100% | Too easy -- solved on first try |
| CASCADE-009 (v2 hardened) | 8 | OTel Demo | ~100% | Still solved -- 8 hops not enough |
| CASCADE-012 (v1) | 30 | OTel Demo | ~40% | Found 2/5 branches, wrong mechanism |
| CASCADE-012 (v2) | 30 | OTel Demo (hardened) | ~35% | Missed temporal trigger, no Slack use |
| CASCADE-012 (v3) | 30 | Train Ticket (1657 files) | ~25% | Fixed notification service only |
| CASCADE-012 (v4) | 30 | Train Ticket (hardened) | ~70% | Found Kafka root cause (best run) |
| CASCADE-012 (v5) | 30 | Train Ticket (7 amplifiers) | ~60% | Found root cause but coworkers still confirmed |
| CASCADE-012 (v6) | 30 | Train Ticket (adversarial coworkers) | TBD | No coworker confirms root cause |
| CASCADE-011 (LLM) | 8 | Train Ticket (LLM coworkers + live traffic) | TBD | Triple silence, Sonnet 4.5 personas |

## Key Design Principles

1. **Difficulty is in investigation, not the fix** -- one-line config change, 30-hop investigation
2. **Significant silence** -- the culprit service has NO errors
3. **Each hop crosses a different boundary** -- service, infrastructure, conceptual, temporal, business
4. **Shallow fixes are punished** -- causal structure makes band-aids fail
5. **Evidence difficulty increases toward root cause** -- easy symptoms, hard root cause
6. **Information is discovered, not given** -- sparse task prompt, rich tool landscape
7. **Multiple valid investigation paths** -- grading accounts for different approaches
