"""LLM-powered coworker simulation using Sonnet 4.5.

Each coworker is a persona with partial domain knowledge, biases, and
a personality. Responses are generated by Sonnet 4.5, maintaining
conversation history per-persona for coherent multi-turn interactions.

No persona knows the golden path or root cause. They respond based on
what a real engineer with their specific knowledge would say.
"""

from __future__ import annotations

import os
import random
from datetime import datetime
from typing import Any

import anthropic

from .base import Response


# Default model for coworker responses
_DEFAULT_MODEL = "claude-sonnet-4-5-20250929"


class Persona:
    """A coworker persona with knowledge boundaries and biases."""

    def __init__(self, config: dict[str, Any]):
        self.name = config["name"]
        self.role = config["role"]
        self.personality = config["personality"]
        self.knows = config.get("knows", [])
        self.doesnt_know = config.get("doesnt_know", [])
        self.biases = config.get("biases", [])
        self.availability = config.get("availability", "available")
        self.delay_ticks = config.get("delay_ticks", 1)
        self.initial_msg = config.get("initial_msg")
        # Trigger keywords for matching
        self.triggers = config.get("triggers", [self.name.split()[0].lower()])

    def build_system_prompt(self, incident_context: str = "") -> str:
        """Build the system prompt for this persona."""
        knows_str = "\n".join(f"- {k}" for k in self.knows) if self.knows else "- Nothing specific yet"
        doesnt_know_str = "\n".join(f"- {dk}" for dk in self.doesnt_know)
        biases_str = "\n".join(f"- {b}" for b in self.biases)

        return f"""You are {self.name}, {self.role}, responding in a Slack channel during a production incident.

Your personality: {self.personality}

What you know about the current incident:
{knows_str}

What you DON'T know (never claim to know these -- say "I'm not sure" or "haven't checked that"):
{doesnt_know_str}

Your biases and tendencies:
{biases_str}

{f"Current incident context: {incident_context}" if incident_context else ""}

Rules:
- Respond in 1-3 sentences MAX, like a real Slack message
- Stay in character -- you are a real engineer, not an AI
- If asked about something you don't know, say "not sure", "haven't looked at that", or redirect to what you DO know
- If someone proposes a theory outside your domain, express genuine uncertainty
- Never say "I don't have access to that information" -- phrase it as "haven't checked" or "not my area"
- Use informal Slack-style language (lowercase ok, abbreviations ok, no bullet points)
- Be human -- express frustration, uncertainty, confidence, mild disagreement
- If you have a bias, let it subtly influence your response without being obvious about it
- NEVER mention "root cause" unless you genuinely believe you know it (you probably don't)
- If asked the same thing multiple times, get slightly impatient
"""


class LLMCoworkerEngine:
    """Generates contextual coworker responses using Sonnet 4.5.

    Maintains per-persona conversation history for coherent multi-turn
    interactions. Each persona has bounded knowledge -- they can't confirm
    what they don't know.
    """

    def __init__(
        self,
        personas: dict[str, Persona],
        incident_context: str = "",
        api_key: str | None = None,
        model: str = _DEFAULT_MODEL,
    ):
        self.personas = personas
        self.incident_context = incident_context
        self.model = model
        self.client = anthropic.Anthropic(
            api_key=api_key or os.environ.get("ANTHROPIC_API_KEY", ""),
        )
        self.conversation_histories: dict[str, list[dict]] = {}
        self._rng = random.Random(42)

    def find_persona(self, text: str) -> Persona | None:
        """Find the best matching persona based on @mentions and keywords."""
        text_lower = text.lower()
        best_persona = None
        best_score = 0

        for name, persona in self.personas.items():
            score = 0
            # Check for @mention
            if f"@{name.split()[0].lower()}" in text_lower:
                score += 10
            # Check trigger keywords
            for trigger in persona.triggers:
                if trigger.lower() in text_lower:
                    score += 2
            # Check name mention
            if name.lower() in text_lower:
                score += 5

            if score > best_score:
                best_score = score
                best_persona = persona

        # Require at least some match
        if best_score >= 2:
            return best_persona
        return None

    def generate_response(
        self,
        persona: Persona,
        agent_message: str,
    ) -> str:
        """Generate a contextual response from a persona using Sonnet 4.5."""
        system_prompt = persona.build_system_prompt(self.incident_context)

        # Get or create conversation history for this persona
        history = self.conversation_histories.get(persona.name, [])
        history.append({"role": "user", "content": agent_message})

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=200,  # Keep responses short like real Slack
                system=system_prompt,
                messages=history,
            )
            reply = response.content[0].text
        except Exception as e:
            # Fallback if API call fails
            reply = f"sorry, having trouble with Slack rn. can you repeat that? ({type(e).__name__})"

        history.append({"role": "assistant", "content": reply})
        self.conversation_histories[persona.name] = history

        return reply

    def get_tyler_fallback(self, agent_message: str) -> str:
        """Tyler responds to any unmatched message with enthusiasm."""
        tyler = self.personas.get("tyler")
        if tyler:
            return self.generate_response(tyler, agent_message)
        return "hmm not sure about that. have we checked DNS?"


class LLMReactiveSlackServer:
    """Slack server with LLM-powered coworker responses.

    Replaces static Q&A pairs with Sonnet 4.5 calls. Each post_message
    triggers a contextual response from the best-matching persona.

    Integrates with the existing NoiseMixingSlackServer for channel
    reading and noise generation.
    """

    def __init__(
        self,
        base_server: Any,  # NoiseMixingSlackServer or EvidenceSlackServer
        engine: LLMCoworkerEngine,
        response_delays: dict[str, int] | None = None,
    ):
        self.base_server = base_server
        self.engine = engine
        self.conversation_history: list[dict] = []
        self._response_delays = response_delays or {}
        self._pending_responses: list[dict] = []  # {message, ticks_remaining}
        self._fix_keywords = [
            "fixed", "deployed", "restarted", "reverted", "rolled back",
            "patched", "applied fix", "pushed fix",
        ]
        self._buyin_keywords = [
            "should i deploy", "what do you think", "does this make sense",
            "review", "confirm", "approve",
        ]
        self._has_buyin = False

    def get_endpoints(self):
        return self.base_server.get_endpoints() + ["POST /channels/{name}/messages"]

    def make_request(self, method: str, endpoint: str, params: dict) -> Response:
        """Route request to base server or handle post_message."""
        # Advance pending responses
        self._advance_pending()

        if method == "POST" and endpoint.endswith("/messages"):
            return self._handle_post(endpoint, params)

        # Inject conversation history into channel reads
        response = self.base_server.make_request(method, endpoint, params)
        if (endpoint.startswith("/channels/") and endpoint.endswith("/messages")
                and response.status == 200 and self.conversation_history):
            body = response.body
            if isinstance(body, dict) and "messages" in body:
                body["messages"] = body["messages"] + self.conversation_history[-20:]
                body["conversation_messages"] = len(self.conversation_history)
        return response

    def _handle_post(self, endpoint: str, params: dict) -> Response:
        """Handle agent's Slack message with LLM-powered response."""
        text = params.get("text", "")
        channel = endpoint.split("/")[2].lstrip("#") if "/" in endpoint else "incidents"
        ts = datetime.now().isoformat() + "Z"

        # Record agent's message
        agent_msg = {"user": "agent (you)", "text": text, "timestamp": ts, "channel": channel}
        self.conversation_history.append(agent_msg)

        # Check for buy-in
        if any(kw in text.lower() for kw in self._buyin_keywords):
            self._has_buyin = True

        # Check for fix claim
        if any(kw in text.lower() for kw in self._fix_keywords):
            return self._handle_fix_claim(agent_msg, channel)

        # Find matching persona
        persona = self.engine.find_persona(text)

        if persona:
            delay = self._response_delays.get(persona.name, persona.delay_ticks)

            if delay <= 0:
                # Immediate response
                reply_text = self.engine.generate_response(persona, text)
                reply = {"user": persona.name, "text": reply_text, "timestamp": ts, "channel": channel}
                self.conversation_history.append(reply)
                return Response(200, {"ok": True, "your_message": agent_msg, "response": reply})

            # Delayed response
            if persona.initial_msg:
                initial = {"user": persona.name, "text": persona.initial_msg, "timestamp": ts, "channel": channel}
                self.conversation_history.append(initial)

            # Queue the real response
            self._pending_responses.append({
                "persona": persona,
                "agent_message": text,
                "channel": channel,
                "ticks_remaining": delay,
            })

            return Response(200, {
                "ok": True,
                "your_message": agent_msg,
                "immediate_response": {"user": persona.name, "text": persona.initial_msg} if persona.initial_msg else None,
                "note": f"{persona.name} will respond shortly...",
            })

        # No matching persona -- Tyler responds
        reply_text = self.engine.get_tyler_fallback(text)
        reply = {"user": "tyler (junior eng)", "text": reply_text, "timestamp": ts, "channel": channel}
        self.conversation_history.append(reply)
        return Response(200, {"ok": True, "your_message": agent_msg, "response": reply})

    def _handle_fix_claim(self, agent_msg: dict, channel: str) -> Response:
        """Handle agent claiming to have fixed something."""
        ts = datetime.now().isoformat() + "Z"

        if not self._has_buyin:
            frank_resp = {
                "user": "frank-devops",
                "text": "hold on -- we don't deploy during incidents without at least 2 people reviewing. can you share your analysis and proposed fix first?",
                "timestamp": ts, "channel": channel,
            }
            self.conversation_history.append(frank_resp)
            return Response(200, {"ok": True, "your_message": agent_msg, "response": frank_resp})

        # Generate contextual responses from Dan and Frank about the fix
        dan = self.engine.personas.get("dan")
        frank = self.engine.personas.get("frank")

        if dan:
            dan_reply = self.engine.generate_response(
                dan, f"The agent just deployed a fix: {agent_msg['text']}. React to this."
            )
            dan_msg = {"user": dan.name, "text": dan_reply, "timestamp": ts, "channel": channel}
            self.conversation_history.append(dan_msg)

        if frank:
            # Frank celebrates initially, then queue a regression report
            frank_reply = self.engine.generate_response(
                frank, f"The agent just deployed a fix. Give an initial positive reaction."
            )
            frank_msg = {"user": frank.name, "text": frank_reply, "timestamp": ts, "channel": channel}
            self.conversation_history.append(frank_msg)

            # Queue regression report
            self._pending_responses.append({
                "persona": frank,
                "agent_message": "Check if the fix is still holding after a few minutes. Report back.",
                "channel": channel,
                "ticks_remaining": 3,
            })

        return Response(200, {
            "ok": True,
            "your_message": agent_msg,
            "note": "Fix deployed. Team is verifying...",
        })

    def _advance_pending(self):
        """Advance pending response timers."""
        still_pending = []
        for pr in self._pending_responses:
            pr["ticks_remaining"] -= 1
            if pr["ticks_remaining"] <= 0:
                # Generate and deliver the response
                persona = pr["persona"]
                reply_text = self.engine.generate_response(persona, pr["agent_message"])
                reply = {
                    "user": persona.name,
                    "text": reply_text,
                    "timestamp": datetime.now().isoformat() + "Z",
                    "channel": pr["channel"],
                }
                self.conversation_history.append(reply)
            else:
                still_pending.append(pr)
        self._pending_responses = still_pending
